{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Beginner Tutorials - 3. Text and Sequence \n",
    "## 3. Text classification with an RNN\n",
    "\n",
    "### Contents\n",
    "1. Setup input pipeline\n",
    "2. Create the model\n",
    "3. Train the model\n",
    "4. Stack two or more LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\la\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\distributions\\distribution.py:265: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\la\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\distributions\\distribution.py:265: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\la\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\distributions\\bernoulli.py:169: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\la\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\distributions\\bernoulli.py:169: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\la\\Anaconda3\\envs\\tf20\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-55b1e50c3331>\", line 4, in <module>\n",
      "    import tensorflow_datasets as tfds\n",
      "ModuleNotFoundError: No module named 'tensorflow_datasets'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\la\\Anaconda3\\envs\\tf20\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ModuleNotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\la\\Anaconda3\\envs\\tf20\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\la\\Anaconda3\\envs\\tf20\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\la\\Anaconda3\\envs\\tf20\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\la\\Anaconda3\\envs\\tf20\\lib\\inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\la\\Anaconda3\\envs\\tf20\\lib\\inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\la\\Anaconda3\\envs\\tf20\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\la\\Anaconda3\\envs\\tf20\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\la\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\la\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\la\\Anaconda3\\envs\\tf20\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 941, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\la\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\__init__.py\", line 45, in <module>\n",
      "    from . _api.v2 import compat\n",
      "  File \"C:\\Users\\la\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\_api\\v2\\compat\\__init__.py\", line 23, in <module>\n",
      "    from . import v1\n",
      "  File \"C:\\Users\\la\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\__init__.py\", line 74, in <module>\n",
      "    from . import test\n",
      "  File \"C:\\Users\\la\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\test\\__init__.py\", line 16, in <module>\n",
      "    from tensorflow.python.framework.test_util import TensorFlowTestCase as TestCase\n",
      "  File \"C:\\Users\\la\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\test_util.py\", line 52, in <module>\n",
      "    from tensorflow.python import _pywrap_util_port\n",
      "  File \"C:\\Users\\la\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\la\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\la\\Anaconda3\\envs\\tf20\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\la\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\__init__.py\", line 134, in <module>\n",
      "    from tensorflow.python.platform import test\n",
      "  File \"C:\\Users\\la\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\platform\\test.py\", line 34, in <module>\n",
      "    from tensorflow.python.framework.test_util import assert_equal_graph_def\n",
      "ImportError: cannot import name 'assert_equal_graph_def'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.historyp['val'+string])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True, as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda install ipywidgets\n",
    "conda install -c conda-forge ipywidgets\n",
    "jupyter nbextension enable --py widgetsnbextension\n",
    "jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "conda install jupyter\n",
    "\n",
    "다 실행해도 jupyter notebook에서는 실행 불가\n",
    "colab에서는 실행 가능\n",
    "https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/r2/tutorials/text/text_classification_rnn.ipynb#scrollTo=SHRwRoP2nVHX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = info.features['text'].encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfds.core.DatasetInfo(\n",
    "    name='imdb_reviews',\n",
    "    version=0.1.0,\n",
    "    description='Large Movie Review Dataset.\n",
    "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
    "    urls=['http://ai.stanford.edu/~amaas/data/sentiment/'],\n",
    "    features=FeaturesDict({\n",
    "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
    "        'text': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8185>),\n",
    "    }),\n",
    "    total_num_examples=100000,\n",
    "    splits={\n",
    "        'test': 25000,\n",
    "        'train': 25000,\n",
    "        'unsupervised': 50000,\n",
    "    },\n",
    "    supervised_keys=('text', 'label'),\n",
    "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "      month     = {June},\n",
    "      year      = {2011},\n",
    "      address   = {Portland, Oregon, USA},\n",
    "      publisher = {Association for Computational Linguistics},\n",
    "      pages     = {142--150},\n",
    "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "    }\"\"\",\n",
    "    redistribution_info=,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.features['text'] #Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8185>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vocabulary size: {}'.format(tokenizer.vocab_size)) # Vocabulary size: 8185"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_string = 'Tensorflow is cool.'\n",
    "\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print('Tokenized string is {}'.format(tokenized_string))\n",
    "# Tokenized string is [6307, 2327, 4043, 4265, 9, 2724, 7975]\n",
    "\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))\n",
    "# The original string: TensorFlow is cool.\n",
    "\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ts in tokenized_string:\n",
    "    print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6307 ----> Ten\n",
    "2327 ----> sor\n",
    "4043 ----> Fl\n",
    "4265 ----> ow \n",
    "9 ----> is \n",
    "2724 ----> cool\n",
    "7975 ----> ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE, train_dataset.output_shapes)\n",
    "test_dataset = test_dataset.padded_batch(BATCH_SIZE, test_dataset.output_shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create the model\n",
    "\n",
    "- An embedding layer stores one vector per word. \n",
    "- After training (on enough data), words with similar meanings often have similar vectors.\n",
    "- tf.keras.layers.Bidirectional : RNN layer.\n",
    " - propagates the input forward and backwards through the RNN layer and then concatenates the output. This helps the RNN to learn long range dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokenizer.vocab_size, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "colab GPU로 돌려도 172초/Epoch 당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_size(vec, size):\n",
    "    zeros = [0] * (size - len(vec))\n",
    "    vec.extend(zeros)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_predict(sentence, pad):\n",
    "    tokenized_sample_pred_text = tokenizer.encode(sample_pred_text)\n",
    "    # sample_pred_text는 함수 밖의 string\n",
    "    \n",
    "    if pad:\n",
    "        tokenized_sample_pred_text = pad_to_size(tokenized_sample_pred_text, 64)\n",
    "        \n",
    "    predictions = model.predict(tf.expand_dims(tokenized_sample_pred_text, 0))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pred_text = ('The movie was cool. The animation and the graphics \\\n",
    "were out of this world. I would recommend this movie.')\n",
    "predictions = sample_predict(sample_pred_text, pad=False)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'accuracy')\n",
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stack two or more LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokenizer.vocab_size, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
    "        64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset)\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pred_text = ('The movie was cool. The animation and the graphics \\\n",
    "were out of this world. I would recommend this movie.')\n",
    "predictions = sample_predict(sample_pred_text, pad=False)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pred_text = ('The movie was cool. The animation and the graphics \\\n",
    "were out of this world. I would recommend this movie.')\n",
    "predictions = sample_predict(sample_pred_text, pad=True)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'accuracy')\n",
    "plot_graphs(history, 'loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf20] *",
   "language": "python",
   "name": "conda-env-tf20-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

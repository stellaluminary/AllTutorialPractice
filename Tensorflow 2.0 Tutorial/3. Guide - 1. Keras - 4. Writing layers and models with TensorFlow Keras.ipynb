{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Guide - 1. Keras - 4. Writing layers and models with TensorFlow Keras\n",
    "\n",
    "목차\n",
    "\n",
    "1. Setup\n",
    "2. The Layer class\n",
    " 1. A Layers.encapsulate a state (weight) and some computation\n",
    " 2. Best practice: deferring weight creation until the shape of the inputs is known\n",
    " 3. Layers are recursively composable\n",
    " 4. Layers recursively collect losses created during the forward pass\n",
    " 5. You can optionally enable serialization on your layers\n",
    " 6. Privileged Training Argument in the call method\n",
    "3. Building Models\n",
    " 1. The Model class\n",
    " 2. Putting it all together: an end-to-end example\n",
    " 3. Beyond object-oriented development: the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Layer class\n",
    "### 1. A Layers.encapsulate a state (weight) and some computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 4) dtype=float32, numpy=\n",
      "array([[-0.02438336,  0.0277977 , -0.06955899,  0.0714218 ],\n",
      "       [ 0.05887324, -0.01789667, -0.05431752, -0.10398325]],\n",
      "      dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>\n",
      "tf.Tensor(\n",
      "[[ 0.03448988  0.00990104 -0.12387651 -0.03256144]\n",
      " [ 0.03448988  0.00990104 -0.12387651 -0.03256144]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class Linear(layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer() # mearn=0, stddev=0.05, only float types are supported\n",
    "        self.w = tf.Variable(initial_value = w_init(shape=(input_dim, units),\n",
    "                                                   dtype='float32'),\n",
    "                            trainable=True)\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(initial_value = b_init(shape=(units,),\n",
    "                                                   dtype='float32'),\n",
    "                            trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        print(self.w)\n",
    "        print(self.b)\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "x = tf.ones((2,2))\n",
    "linear_layer = Linear(4,2)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(2, 4) dtype=float32, numpy=\n",
       " array([[-0.02438336,  0.0277977 , -0.06955899,  0.0714218 ],\n",
       "        [ 0.05887324, -0.01789667, -0.05431752, -0.10398325]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 4) dtype=float32, numpy=\n",
      "array([[-0.10860167, -0.04979282, -0.06922183,  0.04520001],\n",
      "       [-0.06313514, -0.06170268,  0.05446358, -0.00530851]],\n",
      "      dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>\n",
      "tf.Tensor(\n",
      "[[-0.1717368  -0.11149549 -0.01475825  0.0398915 ]\n",
      " [-0.1717368  -0.11149549 -0.01475825  0.0398915 ]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class Linear(layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = self.add_weight(shape=(input_dim, units),\n",
    "                                 initializer = 'random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(units,),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        print(self.w)\n",
    "        print(self.b)\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "x = tf.ones((2,2))\n",
    "linear_layer = Linear(4,2)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layers can have non-trainable weights\n",
    "\n",
    "Besides trainable weights, you can add non-trainable weights to a layer as well. Such weights are meant not to be taken into account during backpropagation, when you are training the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([2., 2.], dtype=float32)>\n",
      "[2. 2.]\n",
      "<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([2., 2.], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([4., 4.], dtype=float32)>\n",
      "[4. 4.]\n"
     ]
    }
   ],
   "source": [
    "class ComputeSum(layers.Layer):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ComputeSum, self).__init__()\n",
    "        self.total = tf.Variable(initial_value=tf.zeros((input_dim)),\n",
    "                                trainable=False)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        print(self.total)\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        print(self.total)\n",
    "        return self.total\n",
    "    \n",
    "x = tf.ones((2,2))\n",
    "my_sum = ComputeSum(2)\n",
    "y = my_sum(x)\n",
    "print(y.numpy())\n",
    "y = my_sum(x)\n",
    "print(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=115, shape=(2,), dtype=float32, numpy=array([2., 2.], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 1\n",
      "non-trainable weights: 1\n",
      "trainable weights: 0\n"
     ]
    }
   ],
   "source": [
    "print('weights:', len(my_sum.weights))\n",
    "print('non-trainable weights:', len(my_sum.non_trainable_weights))\n",
    "\n",
    "print('trainable weights:', len(my_sum.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([4., 4.], dtype=float32)>]\n",
      "non-trainable weights: [<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([4., 4.], dtype=float32)>]\n",
      "trainable weights: 0\n"
     ]
    }
   ],
   "source": [
    "print('weights:', my_sum.weights)\n",
    "print('non-trainable weights:', my_sum.non_trainable_weights)\n",
    "\n",
    "print('trainable weights:', len(my_sum.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Best practice: deferring weight creation until the shape of the inputs is known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = self.add_weight(shape=(input_dim, units),\n",
    "                                initializer='random_normal',\n",
    "                                trainable=True)\n",
    "        self.b = self.add_weight(shape=(units,),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                initializer='random_normal',\n",
    "                                trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.00549616 -0.00095546 -0.13881332  0.11873589 -0.10616826  0.00998187\n",
      "  -0.04270286  0.06722576  0.05021397 -0.07095472  0.04934657 -0.05055718\n",
      "  -0.02400014 -0.07064343 -0.02878763  0.03326192 -0.11734097  0.11738516\n",
      "   0.10818116  0.11819483  0.00040014  0.03704372  0.17260377 -0.04371251\n",
      "   0.11862691 -0.08412854  0.10576034 -0.00344074 -0.06683973  0.00077652\n",
      "  -0.04206975 -0.00267351]\n",
      " [-0.00549616 -0.00095546 -0.13881332  0.11873589 -0.10616826  0.00998187\n",
      "  -0.04270286  0.06722576  0.05021397 -0.07095472  0.04934657 -0.05055718\n",
      "  -0.02400014 -0.07064343 -0.02878763  0.03326192 -0.11734097  0.11738516\n",
      "   0.10818116  0.11819483  0.00040014  0.03704372  0.17260377 -0.04371251\n",
      "   0.11862691 -0.08412854  0.10576034 -0.00344074 -0.06683973  0.00077652\n",
      "  -0.04206975 -0.00267351]], shape=(2, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.ones((2,2))\n",
    "linear_layer = Linear(32)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Layers are recursively composable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 6\n",
      "trainable_weights: 6\n",
      "tf.Tensor(\n",
      "[[0.01623996]\n",
      " [0.01623996]\n",
      " [0.01623996]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class MLPBlock(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(1)\n",
    "    def call(self,inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)\n",
    "    \n",
    "mlp = MLPBlock()\n",
    "y = mlp(tf.ones(shape=(3,64)))\n",
    "print('weights:', len(mlp.weights))\n",
    "print('trainable_weights:', len(mlp.trainable_weights))\n",
    "#print('weights:', (mlp.weights))\n",
    "#print('trainable_weights:', (mlp.trainable_weights))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Layers recursively collect losses created during the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivityRegularizationLayer(layers.Layer):\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(ActivityRegularizationLayer, self).__init__()\n",
    "        self.rate = rate\n",
    "    def call(self,inputs):\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OuterLayer(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(OuterLayer, self).__init__()\n",
    "        self.activity_reg = ActivityRegularizationLayer(1e-2)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.activity_reg(inputs)\n",
    "    \n",
    "layer = OuterLayer()\n",
    "assert len(layer.losses) == 0\n",
    "_ = layer(tf.zeros(1,1))\n",
    "assert len(layer.losses) == 1\n",
    "\n",
    "_ = layer(tf.zeros(1,1))\n",
    "assert len(layer.losses) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=246, shape=(), dtype=float32, numpy=0.0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=284, shape=(), dtype=float32, numpy=0.0019819872>]\n"
     ]
    }
   ],
   "source": [
    "class OuterLayer(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(OuterLayer, self).__init__()\n",
    "        self.dense = layers.Dense(32, kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "    \n",
    "layer = OuterLayer()\n",
    "_ = layer(tf.zeros((1,1)))\n",
    "\n",
    "print(layer.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "optimizer = tf.keras.optimizers.SGD(1e-3)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "for x_batch_train, y_batch_train in train_dataset:\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = layer(x_batch_train)\n",
    "        loss_value = loss_fn(y_batch_train, logits)\n",
    "        loss_value += sum(model.losses)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. You can optionally enable serialization on your layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'units': 64}\n"
     ]
    }
   ],
   "source": [
    "class Linear(layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                initializer='random_normal',\n",
    "                                trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                initializer='random_normal',\n",
    "                                trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {'units' : self.units}\n",
    "    \n",
    "layer = Linear(64)\n",
    "config = layer.get_config()\n",
    "print(config)\n",
    "new_layer = Linear.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'linear_8', 'trainable': True, 'dtype': None, 'units': 64}\n"
     ]
    }
   ],
   "source": [
    "class Linear(layers.Layer):\n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                initializer='random_normal',\n",
    "                                trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                initializer='random_normal',\n",
    "                                trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(Linear, self).get_config()\n",
    "        config.update({'units' : self.units})\n",
    "        return config\n",
    "    \n",
    "layer = Linear(64)\n",
    "config = layer.get_config()\n",
    "print(config)\n",
    "new_layer = Linear.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_config(cls, config):\n",
    "    return cls(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Privileged Training Argument in the call method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDropout(layers.Layer):\n",
    "    def __init__(self, rate, **kwargs):\n",
    "        super(CustomDropout, self).__init__(**kwargs)\n",
    "        self.rate = rate\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            return tf.nn.dropout(inputs, rate=self.rate)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building Models\n",
    "### 1. The Model class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class ResNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.block_1 = ResNetBlock()\n",
    "        self.block_2 = ResNetBlock()\n",
    "        self.global_pool = layers.GlobalAveragePooling2D()\n",
    "        self.classifier = Dense(num_classes)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.block_1(inputs)\n",
    "        x = self.block_2(x)\n",
    "        x = self.global_pool(x)\n",
    "        return self.classifier(x)\n",
    "    \n",
    "resnet = ResNet()\n",
    "dataset = ...\n",
    "resnet.fit(dataset, epochs=10)\n",
    "resnet.save_weights(filepath)        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Putting it all together: an end-to-end example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "step 0: mean loss = tf.Tensor(0.33883977, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(0.124884464, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(0.09889148, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(0.08896656, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(0.084094346, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(0.080759406, shape=(), dtype=float32)\n",
      "step 600: mean loss = tf.Tensor(0.0786373, shape=(), dtype=float32)\n",
      "step 700: mean loss = tf.Tensor(0.07708535, shape=(), dtype=float32)\n",
      "step 800: mean loss = tf.Tensor(0.07590893, shape=(), dtype=float32)\n",
      "step 900: mean loss = tf.Tensor(0.0749209, shape=(), dtype=float32)\n",
      "Start of epoch 1\n",
      "step 0: mean loss = tf.Tensor(0.07460664, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(0.07395489, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(0.073460914, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(0.072985545, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(0.07266389, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(0.072262764, shape=(), dtype=float32)\n",
      "step 600: mean loss = tf.Tensor(0.071963586, shape=(), dtype=float32)\n",
      "step 700: mean loss = tf.Tensor(0.07168981, shape=(), dtype=float32)\n",
      "step 800: mean loss = tf.Tensor(0.07144489, shape=(), dtype=float32)\n",
      "step 900: mean loss = tf.Tensor(0.071194895, shape=(), dtype=float32)\n",
      "Start of epoch 2\n",
      "step 0: mean loss = tf.Tensor(0.071109325, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(0.07093712, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(0.070803, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(0.070652746, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(0.070560016, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(0.07040352, shape=(), dtype=float32)\n",
      "step 600: mean loss = tf.Tensor(0.07028831, shape=(), dtype=float32)\n",
      "step 700: mean loss = tf.Tensor(0.07017639, shape=(), dtype=float32)\n",
      "step 800: mean loss = tf.Tensor(0.070071064, shape=(), dtype=float32)\n",
      "step 900: mean loss = tf.Tensor(0.0699544, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch,dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self,\n",
    "                latent_dim=32,\n",
    "                intermediate_dim=64,\n",
    "                name='encoder',\n",
    "                **kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z\n",
    "    \n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self,\n",
    "                original_dim,\n",
    "                intermediate_dim=64,\n",
    "                name='decoder',\n",
    "                **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\n",
    "        self.dense_output = layers.Dense(original_dim, activation='sigmoid')\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        return self.dense_output(x)\n",
    "    \n",
    "class VariationalAutoEncoder(tf.keras.Model):\n",
    "    def __init__(self, \n",
    "                 original_dim,\n",
    "                 intermediate_dim=64,\n",
    "                 latent_dim=32,\n",
    "                 name='autoencoder',\n",
    "                 **kwargs):\n",
    "        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim = latent_dim,\n",
    "                              intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        kl_loss = -0.5 *tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed\n",
    "    \n",
    "original_dim = 784\n",
    "vae = VariationalAutoEncoder(original_dim, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 1024).batch(64)\n",
    "\n",
    "for epoch in range(3):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "    \n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed = vae(x_batch_train)\n",
    "            loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "            loss += sum(vae.losses)\n",
    "            \n",
    "        grads = tape.gradient(loss, vae.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "        \n",
    "        loss_metric(loss)\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print('step %s: mean loss = %s' %(step, loss_metric.result()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0747\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0676\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2615fdaaa58>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VariationalAutoEncoder(784, 64,32)\n",
    "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(x_train, x_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Beyond object-oriented development: the Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0747\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0676\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2627ab5b6a0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_dim = 784\n",
    "intermediate_dim = 64\n",
    "latent_dim = 32\n",
    "\n",
    "original_inputs = tf.keras.Input(shape=(original_dim,), name='encoder_input')\n",
    "x = layers.Dense(intermediate_dim, activation='relu')(original_inputs)\n",
    "z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "z = Sampling()((z_mean, z_log_var))\n",
    "encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name='encoder')\n",
    "\n",
    "latent_inputs = tf.keras.Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = layers.Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = layers.Dense(original_dim, activation='sigmoid')(x)\n",
    "decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name='decoder')\n",
    "\n",
    "outputs = decoder(z)\n",
    "vae = tf.keras.Model(inputs=original_inputs, outputs=outputs, name='vae')\n",
    "\n",
    "kl_loss = - 0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "vae.add_loss(kl_loss)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(x_train, x_train, epochs=3, batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf20a",
   "language": "python",
   "name": "tf20a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

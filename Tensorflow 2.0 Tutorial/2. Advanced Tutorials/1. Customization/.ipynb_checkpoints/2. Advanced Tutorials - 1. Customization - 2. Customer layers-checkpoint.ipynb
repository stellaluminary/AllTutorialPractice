{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Advanced Tutorials - 1. Customization - 2. Customer layers\n",
    "\n",
    "URL : https://www.tensorflow.org/beta/tutorials/eager/custom_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend using tf.keras as a high-level API for building neural networks. <br>\n",
    "That said, most TensorFlow APIs are usable with eager execution.<br>\n",
    "\n",
    "### 목차\n",
    "1. Layers:common sets of useful operations\n",
    "2. Implementing custom layers\n",
    "3. Models:composing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Layers:common sets of useful operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow includes the full Keras API in the tf.keras package, and the Keras layers are very useful when building your own models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most layers take as a first argument the number of output dimensions / channels.\n",
    "layer = tf.keras.layers.Dense(100)\n",
    "# The number of input dimensions is often unnecessary\n",
    "layer = tf.keras.layers.Dense(10, input_shape=(None,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full list of pre-existing layers can be seen in the documentation. <br>\n",
    "It includes Dense (a fully-connected layer), Conv2D, LSTM, BatchNormalization, Dropout, and many others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=29, shape=(10, 10), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(tf.zeros([10,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_1/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
       " array([[ 0.2280271 , -0.60855603,  0.15571153, -0.40853864, -0.01158875,\n",
       "          0.31229687,  0.4122607 , -0.2299824 ,  0.6106041 ,  0.1563198 ],\n",
       "        [ 0.07169735, -0.512296  , -0.21175817,  0.46712393,  0.37026936,\n",
       "          0.22550094,  0.5880677 ,  0.33798647, -0.44650972, -0.01501089],\n",
       "        [ 0.30120134, -0.4717366 ,  0.50412863,  0.29858047,  0.3203612 ,\n",
       "          0.15965658,  0.48445624, -0.41152123,  0.0111109 , -0.18924671],\n",
       "        [ 0.4724663 ,  0.02966458, -0.29876322, -0.5243287 ,  0.3432626 ,\n",
       "          0.40037566, -0.09515655,  0.37963063,  0.04484695, -0.3192841 ],\n",
       "        [-0.30729488,  0.48230678, -0.12428796,  0.31491196,  0.23150897,\n",
       "          0.31834316,  0.10172778, -0.0533343 , -0.47070235, -0.42248136]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_1/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
       " array([[ 0.2280271 , -0.60855603,  0.15571153, -0.40853864, -0.01158875,\n",
       "          0.31229687,  0.4122607 , -0.2299824 ,  0.6106041 ,  0.1563198 ],\n",
       "        [ 0.07169735, -0.512296  , -0.21175817,  0.46712393,  0.37026936,\n",
       "          0.22550094,  0.5880677 ,  0.33798647, -0.44650972, -0.01501089],\n",
       "        [ 0.30120134, -0.4717366 ,  0.50412863,  0.29858047,  0.3203612 ,\n",
       "          0.15965658,  0.48445624, -0.41152123,  0.0111109 , -0.18924671],\n",
       "        [ 0.4724663 ,  0.02966458, -0.29876322, -0.5243287 ,  0.3432626 ,\n",
       "          0.40037566, -0.09515655,  0.37963063,  0.04484695, -0.3192841 ],\n",
       "        [-0.30729488,  0.48230678, -0.12428796,  0.31491196,  0.23150897,\n",
       "          0.31834316,  0.10172778, -0.0533343 , -0.47070235, -0.42248136]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'dense_1/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
       " array([[ 0.2280271 , -0.60855603,  0.15571153, -0.40853864, -0.01158875,\n",
       "          0.31229687,  0.4122607 , -0.2299824 ,  0.6106041 ,  0.1563198 ],\n",
       "        [ 0.07169735, -0.512296  , -0.21175817,  0.46712393,  0.37026936,\n",
       "          0.22550094,  0.5880677 ,  0.33798647, -0.44650972, -0.01501089],\n",
       "        [ 0.30120134, -0.4717366 ,  0.50412863,  0.29858047,  0.3203612 ,\n",
       "          0.15965658,  0.48445624, -0.41152123,  0.0111109 , -0.18924671],\n",
       "        [ 0.4724663 ,  0.02966458, -0.29876322, -0.5243287 ,  0.3432626 ,\n",
       "          0.40037566, -0.09515655,  0.37963063,  0.04484695, -0.3192841 ],\n",
       "        [-0.30729488,  0.48230678, -0.12428796,  0.31491196,  0.23150897,\n",
       "          0.31834316,  0.10172778, -0.0533343 , -0.47070235, -0.42248136]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.kernel, layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementing custom layers\n",
    "\n",
    "The best way to implement your own layer is extending the tf.keras.Layer class and implementing:\n",
    "- \\* __\\_\\_init\\_\\___ , where you can do all input-independent initialization \n",
    "- \\* __build__, where you know the shapes of the input tensors and can do the rest of the initialization \n",
    "- \\* __call__, where you do the forward computation\n",
    "\n",
    "Note that you don't have to wait until build is called to create your variables, you can also create them in \\_\\_init\\_\\_. <br>\n",
    "However, the advantage of creating them in build is that it enables late variable creation based on the shape of the inputs the layer will operate on. <br>\n",
    "=> enables late variable creation? 뭔 말<br>\n",
    "On the other hand, creating variables in __init__ would mean that shapes required to create the variables will need to be explicitly specified.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n",
      "[<tf.Variable 'my_dense_layer/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
      "array([[-0.31830022,  0.41905528,  0.0314694 , -0.5025718 , -0.07129866,\n",
      "        -0.3728037 ,  0.09434938, -0.27421716,  0.13896817,  0.07145447],\n",
      "       [-0.12444615,  0.07562363, -0.01160127, -0.3638782 , -0.15126851,\n",
      "        -0.10554862,  0.34031224, -0.5281734 , -0.05240583,  0.33716607],\n",
      "       [ 0.41005737, -0.45011872,  0.4481482 ,  0.1853888 ,  0.5341503 ,\n",
      "         0.24707466,  0.3368954 ,  0.36519325, -0.17926222,  0.0956136 ],\n",
      "       [-0.28885728, -0.42473686, -0.28443643,  0.1492737 , -0.44854555,\n",
      "         0.59450597, -0.3328274 , -0.59326017, -0.05818152,  0.56307596],\n",
      "       [ 0.475277  , -0.09537458,  0.24648327,  0.17749107, -0.1839602 ,\n",
      "        -0.23916909,  0.4369462 , -0.48611948,  0.6057226 , -0.1259762 ]],\n",
      "      dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_outputs):\n",
    "        super(MyDenseLayer, self).__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_variable('kernel', shape=[int(input_shape[-1]), self.num_outputs])\n",
    "        \n",
    "    def call(self, input):\n",
    "        return tf.matmul(input, self.kernel)\n",
    "    \n",
    "layer = MyDenseLayer(10)\n",
    "print(layer(tf.zeros([10,5])))\n",
    "print(layer.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Models:composing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetIdentityBlock(tf.keras.Model):\n",
    "    def __init__(self, kernel_size, filters):\n",
    "        super(ResnetIdentityBlock, self).__init__()\n",
    "        filters1, filters2, filters3 = filters\n",
    "        \n",
    "        self.conv2a = tf.keras.layers.Conv2D(filters1, (1,1))\n",
    "        self.bn2a = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size, padding='same')\n",
    "        self.bn2b = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv2c = tf.keras.layers.Conv2D(filters3, (1,1))\n",
    "        self.bn2c = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.conv2a(input_tensor)\n",
    "        x = self.bn2a(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.conv2b(x)\n",
    "        x = self.bn2b(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv2c(x)\n",
    "        x = self.bn2c(x, training=training)\n",
    "        \n",
    "        x += input_tensor\n",
    "        return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]], shape=(1, 2, 3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "block = ResnetIdentityBlock(1, [1,2,3])\n",
    "print(block(tf.zeros([1,2,3,3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['resnet_identity_block/conv2d/kernel:0', 'resnet_identity_block/conv2d/bias:0', 'resnet_identity_block/batch_normalization_v2/gamma:0', 'resnet_identity_block/batch_normalization_v2/beta:0', 'resnet_identity_block/conv2d_1/kernel:0', 'resnet_identity_block/conv2d_1/bias:0', 'resnet_identity_block/batch_normalization_v2_1/gamma:0', 'resnet_identity_block/batch_normalization_v2_1/beta:0', 'resnet_identity_block/conv2d_2/kernel:0', 'resnet_identity_block/conv2d_2/bias:0', 'resnet_identity_block/batch_normalization_v2_2/gamma:0', 'resnet_identity_block/batch_normalization_v2_2/beta:0']\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "a = [x.name for x in block.trainable_variables]\n",
    "print(a)\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of the time, however, models which compose many layers simply call one layer after the other. <br>\n",
    "This can be done in very little code using tf.keras.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=737, shape=(1, 2, 3, 3), dtype=float32, numpy=\n",
       "array([[[[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_seq = tf.keras.Sequential([tf.keras.layers.Conv2D(1, (1,1), input_shape=(None, None, 3)),\n",
    "                             tf.keras.layers.BatchNormalization(),\n",
    "                             tf.keras.layers.Conv2D(2, 1,\n",
    "                                                    padding='same'),\n",
    "                             tf.keras.layers.BatchNormalization(),\n",
    "                             tf.keras.layers.Conv2D(3, (1, 1)),\n",
    "                             tf.keras.layers.BatchNormalization()])\n",
    "my_seq(tf.zeros([1,2,3,3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf20a",
   "language": "python",
   "name": "tf20a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Tutorials - 1. Customization \n",
    "# 5. TF function and AutoGraph\n",
    "1. State inft.function\n",
    "- Variables\n",
    "- Control flow and autograph\n",
    "- Next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=16, shape=(2, 2), dtype=float32, numpy=\n",
       "array([[2., 2.],\n",
       "       [2., 2.]], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.function\n",
    "\n",
    "def add(a,b):\n",
    "    return a+b\n",
    "\n",
    "add(tf.ones([2,2]), tf.ones([2,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=45, shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.function\n",
    "\n",
    "def add(a,b):\n",
    "    return a+b\n",
    "\n",
    "v=tf.Variable(1.)\n",
    "with tf.GradientTape() as tape:\n",
    "    result = add(v,1.)\n",
    "tape.gradient(result,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=75, shape=(3, 2), dtype=float32, numpy=\n",
       "array([[3., 3.],\n",
       "       [3., 3.],\n",
       "       [3., 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.function\n",
    "def dense_layer(x,w,b):\n",
    "    return add(tf.matmul(x,w),b)\n",
    "\n",
    "dense_layer(tf.ones([3,2]), tf.ones([2,2]), tf.ones([2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polymorphism\n",
    "다형성 (컴퓨터 과학)\n",
    "\n",
    "프로그램 언어의 다형성(多形性, polymorphism; 폴리모피즘)은 그 프로그래밍 언어의 자료형 체계의 성질을 나타내는 것으로, 프로그램 언어의 각 요소들(상수, 변수, 식, 오브젝트, 함수, 메소드 등)이 다양한 자료형(type)에 속하는 것이 허가되는 성질을 가리킨다. 반댓말은 단형성(monomorphism)으로, 프로그램 언어의 각 요소가 한가지 형태만 가지는 성질을 가리킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add 1 tf.Tensor(2, shape=(), dtype=int32)\n",
      "add 1.1 tf.Tensor(2.2, shape=(), dtype=float32)\n",
      "add string tensor tf.Tensor(b'aa', shape=(), dtype=string)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=105, shape=(), dtype=string, numpy=b'aa'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.function\n",
    "def add(a):\n",
    "    return a+a\n",
    "\n",
    "print('add 1', add(1))\n",
    "print('add 1.1', add(1.1))\n",
    "print('add string tensor', add(tf.constant('a')))\n",
    "c=add.get_concrete_function(tf.TensorSpec(shape=None, dtype=tf.string))\n",
    "c(a=tf.constant('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager conv: 0.0028976\n",
      "Function conv: 0.0030265\n",
      "Note how there's not much difference in performance for convolutions\n"
     ]
    }
   ],
   "source": [
    "# Functions can be faster than eager code, for graphs with many small ops\n",
    "import timeit\n",
    "conv_layer = tf.keras.layers.Conv2D(100,3)\n",
    "\n",
    "@tf.function\n",
    "def conv_fn(image):\n",
    "    return conv_layer(image)\n",
    "\n",
    "image = tf.zeros([1,200,200,100])\n",
    "conv_layer(image); conv_fn(image)\n",
    "print('Eager conv:', timeit.timeit(lambda: conv_layer(image), number=10))\n",
    "print('Function conv:', timeit.timeit(lambda: conv_fn(image), number=10))\n",
    "print(\"Note how there's not much difference in performance for convolutions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager lstm 0.05195259999999999\n",
      "Function lstm 0.005350400000000005\n"
     ]
    }
   ],
   "source": [
    "lstm_cell = tf.keras.layers.LSTMCell(10)\n",
    "\n",
    "@tf.function\n",
    "def lstm_fn(input, state):\n",
    "    return lstm_cell(input, state)\n",
    "\n",
    "input=tf.zeros([10,10])\n",
    "state = [tf.zeros([10,10])]*2\n",
    "lstm_cell(input,state); lstm_fn(input,state)\n",
    "print('Eager lstm', timeit.timeit(lambda: lstm_cell(input,state), number=10))\n",
    "print('Function lstm', timeit.timeit(lambda: lstm_fn(input,state), number=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. State in tf.function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1611, shape=(), dtype=float32, numpy=10.0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=tf.Variable(1.)\n",
    "b=tf.Variable(2.)\n",
    "\n",
    "@tf.function\n",
    "def f(x,y):\n",
    "    a.assign(y**b)\n",
    "    b.assign_add(x*a)\n",
    "    return a+b\n",
    "\n",
    "f(1.,2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Variables\n",
    "\n",
    "leverage 이점, caveat 경고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "@tf.function\n",
    "def f(x):\n",
    "    v = tf.Variable(1.)\n",
    "    v.assign_add(x)\n",
    "    return v\n",
    "\n",
    "f(1.)\n",
    "\n",
    "# error \n",
    "ValueError: tf.function-decorated function tried to create variables on non-first call.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1636, shape=(), dtype=float32, numpy=4.0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.Variable(1.0)\n",
    "\n",
    "@tf.function\n",
    "def f(x):\n",
    "    return v.assign_add(x)\n",
    "\n",
    "f(1.0)  # 2.0\n",
    "f(2.0)  # 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.0, shape=(), dtype=float32)\n",
      "tf.Tensor(4.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class C:pass\n",
    "obj = C(); obj.v = None\n",
    "\n",
    "@tf.function\n",
    "def g(x):\n",
    "    if obj.v is None:\n",
    "        obj.v = tf.Variable(1.)\n",
    "    return obj.v.assign_add(x)\n",
    "\n",
    "print(g(1.))\n",
    "print(g(2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0419 14:15:59.214346 11796 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x000002537C8AF4A8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
      "W0419 14:15:59.219303 11796 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x000002537C8A2688> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x000002537C8AF4A8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x000002537C8A2688> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1798, shape=(), dtype=float32, numpy=36.0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state =[]\n",
    "@tf.function\n",
    "def fn(x):\n",
    "    if not state:\n",
    "        state.append(tf.Variable(2.*x))\n",
    "        state.append(tf.Variable(state[0] *3))\n",
    "    return state[0] * x * state[1]\n",
    "\n",
    "fn(tf.constant(1.))\n",
    "fn(tf.constant(3.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Control flow and autograph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0125720501 0.939365625 0.966633201 ... 0.0132431984 0.111525059 0.306486726]\n",
      "[0.0125713879 0.734930634 0.747221 ... 0.0132424245 0.11106497 0.297237575]\n",
      "[0.0125707258 0.626072586 0.633488059 ... 0.0132416505 0.110610537 0.288782567]\n",
      "[0.0125700636 0.555341959 0.560449362 ... 0.0132408766 0.11016164 0.281013906]\n",
      "[0.0125694014 0.504513144 0.508310735 ... 0.0132401027 0.109718174 0.273843199]\n",
      "[0.0125687392 0.465659082 0.468627959 ... 0.0132393287 0.10928002 0.267197281]\n",
      "[0.0125680771 0.43468529 0.437090069 ... 0.0132385548 0.108847082 0.261014968]\n",
      "[0.0125674158 0.409229428 0.411229521 ... 0.0132377818 0.108419247 0.255244613]\n",
      "[0.0125667546 0.387818217 0.389516175 ... 0.0132370088 0.107996427 0.249842301]\n",
      "[0.0125660934 0.369477808 0.37094304 ... 0.0132362358 0.107578516 0.244770408]\n",
      "[0.0125654321 0.353534877 0.354816288 ... 0.0132354628 0.107165426 0.239996508]\n",
      "[0.0125647709 0.339506716 0.340639919 ... 0.0132346898 0.10675706 0.235492453]\n",
      "[0.0125641096 0.327036917 0.328048557 ... 0.0132339168 0.106353328 0.231233686]\n",
      "[0.0125634484 0.315855801 0.316766232 ... 0.0132331448 0.105954148 0.22719869]\n",
      "[0.0125627872 0.3057549 0.30658 ... 0.0132323727 0.105559431 0.223368481]\n",
      "[0.0125621259 0.296570241 0.297322601 ... 0.0132316006 0.105169095 0.219726279]\n",
      "[0.0125614656 0.288170785 0.2888605 ... 0.0132308286 0.104783066 0.216257155]\n",
      "[0.0125608053 0.280450314 0.28108567 ... 0.0132300565 0.104401261 0.212947801]\n",
      "[0.012560145 0.273321807 0.273909599 ... 0.0132292844 0.104023598 0.209786311]\n",
      "[0.0125594847 0.266713053 0.267258942 ... 0.0132285124 0.103650011 0.206761956]\n",
      "[0.0125588244 0.260563672 0.261072427 ... 0.0132277412 0.103280418 0.203865096]\n",
      "[0.0125581641 0.254822671 0.255298346 ... 0.0132269701 0.102914758 0.201087]\n",
      "[0.0125575038 0.24944666 0.249892682 ... 0.013226199 0.102552958 0.19841975]\n",
      "[0.0125568435 0.244398445 0.244817778 ... 0.0132254278 0.10219495 0.195856169]\n",
      "[0.0125561832 0.239645943 0.240041152 ... 0.0132246567 0.101840667 0.193389684]\n",
      "[0.0125555238 0.235161304 0.235534623 ... 0.0132238856 0.101490043 0.19101432]\n",
      "[0.0125548644 0.230920225 0.231273606 ... 0.0132231144 0.101143017 0.188724592]\n",
      "[0.012554205 0.226901382 0.227236539 ... 0.0132223442 0.100799531 0.186515465]\n",
      "[0.0125535456 0.223085985 0.223404437 ... 0.013221574 0.100459523 0.184382319]\n",
      "[0.0125528863 0.219457403 0.219760492 ... 0.0132208038 0.100122936 0.182320878]\n",
      "[0.0125522269 0.21600084 0.216289774 ... 0.0132200336 0.0997897089 0.180327222]\n",
      "[0.0125515675 0.212703109 0.212978944 ... 0.0132192634 0.0994597897 0.1783977]\n",
      "[0.0125509081 0.209552377 0.209816083 ... 0.0132184932 0.0991331264 0.176528946]\n",
      "[0.0125502488 0.206538022 0.206790462 ... 0.013217723 0.0988096595 0.174717829]\n",
      "[0.0125495903 0.20365046 0.20389241 ... 0.0132169537 0.0984893441 0.172961444]\n",
      "[0.0125489319 0.200881034 0.201113209 ... 0.0132161845 0.0981721207 0.171257094]\n",
      "[0.0125482734 0.198221892 0.198444933 ... 0.0132154152 0.0978579447 0.169602245]\n",
      "[0.012547615 0.195665896 0.195880383 ... 0.0132146459 0.0975467712 0.167994544]\n",
      "[0.0125469565 0.193206519 0.193413 ... 0.0132138766 0.0972385481 0.1664318]\n",
      "[0.0125462981 0.19083783 0.191036791 ... 0.0132131074 0.0969332308 0.164911941]\n",
      "[0.0125456396 0.188554376 0.188746259 ... 0.0132123381 0.0966307744 0.16343306]\n",
      "[0.0125449812 0.186351165 0.186536372 ... 0.0132115697 0.0963311344 0.161993325]\n",
      "[0.0125443228 0.184223592 0.184402511 ... 0.0132108014 0.0960342661 0.160591051]\n",
      "[0.0125436652 0.182167426 0.182340398 ... 0.0132100331 0.0957401246 0.159224629]\n",
      "[0.0125430077 0.180178747 0.180346102 ... 0.0132092647 0.0954486728 0.157892555]\n",
      "[0.0125423502 0.178253949 0.178415984 ... 0.0132084964 0.0951598659 0.156593427]\n",
      "[0.0125416927 0.176389664 0.176546663 ... 0.013207728 0.0948736668 0.15532589]\n",
      "[0.0125410352 0.174582794 0.17473501 ... 0.0132069597 0.0945900381 0.154088691]\n",
      "[0.0125403777 0.172830448 0.172978118 ... 0.0132061923 0.0943089351 0.152880639]\n",
      "[0.0125397202 0.171129927 0.171273276 ... 0.0132054249 0.0940303281 0.151700601]\n",
      "[0.0125390626 0.169478729 0.169617966 ... 0.0132046575 0.0937541798 0.150547519]\n",
      "[0.0125384051 0.167874515 0.168009818 ... 0.0132038901 0.0934804529 0.149420381]\n",
      "[0.0125377486 0.166315094 0.166446656 ... 0.0132031227 0.0932091102 0.148318216]\n",
      "[0.012537092 0.164798409 0.164926395 ... 0.0132023552 0.0929401144 0.147240117]\n",
      "[0.0125364354 0.163322553 0.163447127 ... 0.0132015878 0.0926734358 0.146185234]\n",
      "[0.0125357788 0.161885723 0.162007019 ... 0.0132008214 0.0924090445 0.145152733]\n",
      "[0.0125351222 0.160486221 0.160604388 ... 0.0132000549 0.0921469 0.144141838]\n",
      "[0.0125344656 0.159122452 0.159237623 ... 0.0131992884 0.0918869823 0.143151805]\n",
      "[0.0125338091 0.157792926 0.157905236 ... 0.0131985219 0.0916292444 0.142181918]\n",
      "[0.0125331525 0.156496242 0.156605795 ... 0.0131977554 0.0913736671 0.141231507]\n",
      "[0.0125324959 0.155231059 0.15533796 ... 0.013196989 0.0911202207 0.140299931]\n",
      "[0.0125318402 0.15399611 0.154100478 ... 0.0131962225 0.0908688679 0.139386564]\n",
      "[0.0125311846 0.152790219 0.152892157 ... 0.0131954569 0.0906195864 0.138490841]\n",
      "[0.0125305289 0.151612267 0.151711866 ... 0.0131946914 0.0903723463 0.137612179]\n",
      "[0.0125298733 0.150461182 0.150558531 ... 0.0131939258 0.0901271179 0.136750057]\n",
      "[0.0125292176 0.149335966 0.149431139 ... 0.0131931603 0.0898838788 0.135903955]\n",
      "[0.012528562 0.148235664 0.148328736 ... 0.0131923947 0.0896426 0.135073379]\n",
      "[0.0125279063 0.147159368 0.147250414 ... 0.0131916292 0.0894032568 0.134257868]\n",
      "[0.0125272507 0.146106213 0.146195307 ... 0.0131908637 0.0891658217 0.133456975]\n",
      "[0.012526595 0.145075381 0.145162597 ... 0.013190099 0.0889302641 0.132670254]\n",
      "[0.0125259403 0.144066095 0.144151494 ... 0.0131893344 0.0886965692 0.1318973]\n",
      "[0.0125252856 0.143077612 0.143161252 ... 0.0131885698 0.0884647071 0.131137729]\n",
      "[0.0125246309 0.142109215 0.142191172 ... 0.0131878052 0.0882346556 0.130391136]\n",
      "[0.0125239762 0.14116025 0.141240582 ... 0.0131870406 0.0880063921 0.129657164]\n",
      "[0.0125233214 0.140230075 0.140308827 ... 0.013186276 0.087779887 0.128935471]\n",
      "[0.0125226667 0.139318064 0.139395297 ... 0.0131855113 0.0875551254 0.128225699]\n",
      "[0.012522012 0.138423651 0.138499394 ... 0.0131847477 0.0873320848 0.127527535]\n",
      "[0.0125213573 0.137546271 0.137620568 ... 0.013183984 0.0871107355 0.126840666]\n",
      "[0.0125207026 0.136685371 0.136758283 ... 0.0131832203 0.0868910626 0.126164794]\n",
      "[0.0125200488 0.135840461 0.135912031 ... 0.0131824566 0.0866730437 0.125499621]\n",
      "[0.012519395 0.135011047 0.135081321 ... 0.0131816929 0.0864566639 0.124844871]\n",
      "[0.0125187412 0.134196669 0.134265676 ... 0.0131809292 0.0862418935 0.124200277]\n",
      "[0.0125180874 0.133396864 0.133464649 ... 0.0131801656 0.0860287175 0.12356557]\n",
      "[0.0125174336 0.1326112 0.132677794 ... 0.0131794028 0.0858171135 0.122940503]\n",
      "[0.0125167798 0.131839275 0.131904721 ... 0.01317864 0.0856070668 0.122324839]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1843, shape=(10,), dtype=float32, numpy=\n",
       "array([0.01251613, 0.1310807 , 0.13114502, 0.13095717, 0.1265257 ,\n",
       "       0.11816382, 0.12782206, 0.01317788, 0.08539855, 0.12171834],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.function\n",
    "def f(x):\n",
    "    while tf.reduce_sum(x) > 1:\n",
    "        tf.print(x)\n",
    "        x = tf.tanh(x)\n",
    "    return x\n",
    "\n",
    "f(tf.random.uniform([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import print_function\n",
      "\n",
      "def tf__f(x):\n",
      "  try:\n",
      "    with ag__.function_scope('f'):\n",
      "      do_return = False\n",
      "      retval_ = None\n",
      "\n",
      "      def loop_test(x_1):\n",
      "        with ag__.function_scope('loop_test'):\n",
      "          return ag__.gt(ag__.converted_call('reduce_sum', tf, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=ag__.Feature.ALL, internal_convert_user_code=True), (x_1,), {}), 1)\n",
      "\n",
      "      def loop_body(x_1):\n",
      "        with ag__.function_scope('loop_body'):\n",
      "          with ag__.utils.control_dependency_on_returns(ag__.converted_call('print', tf, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=ag__.Feature.ALL, internal_convert_user_code=True), (x_1,), {})):\n",
      "            x, tf_1 = ag__.utils.alias_tensors(x_1, tf)\n",
      "            x = ag__.converted_call('tanh', tf_1, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=ag__.Feature.ALL, internal_convert_user_code=True), (x,), {})\n",
      "            return x,\n",
      "      x, = ag__.while_stmt(loop_test, loop_body, (x,), (tf, x, ag__))\n",
      "      do_return = True\n",
      "      retval_ = x\n",
      "      return retval_\n",
      "  except:\n",
      "    ag__.rewrite_graph_construction_error(ag_source_map__)\n",
      "\n",
      "\n",
      "\n",
      "tf__f.autograph_info__ = {}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    while tf.reduce_sum(x) > 1:\n",
    "        tf.print(x)\n",
    "        x = tf.tanh(x)\n",
    "    return x\n",
    "\n",
    "print(tf.autograph.to_code(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "@tf.function\n",
    "def f(x):\n",
    "  for i in range(10):  # Static python loop, we'll not convert it\n",
    "    do_stuff()\n",
    "  for i in tf.range(10):  # depends on a tensor, we'll convert it\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "1\n",
      "40\n",
      "2\n",
      "80\n",
      "3\n",
      "160\n",
      "4\n",
      "320\n",
      "5\n",
      "640\n",
      "6\n",
      "1280\n",
      "7\n",
      "2560\n",
      "8\n",
      "5120\n",
      "9\n",
      "10240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1907, shape=(), dtype=int32, numpy=10240>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.function\n",
    "def f(x):\n",
    "    for i in tf.range(10):\n",
    "        tf.print(i)\n",
    "        tf.Assert(i<10, ['a'])\n",
    "        x += x\n",
    "        tf.print(x)\n",
    "    return x\n",
    "\n",
    "f(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1976, shape=(10,), dtype=float32, numpy=\n",
       "array([   20.,    40.,    80.,   160.,   320.,   640.,  1280.,  2560.,\n",
       "        5120., 10240.], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.function\n",
    "def f(x):\n",
    "    ta = tf.TensorArray(tf.float32, size=10)\n",
    "    for i in tf.range(10):\n",
    "        x += x\n",
    "        ta = ta.write(i,x)\n",
    "    return ta.stack()\n",
    "\n",
    "f(10.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf20a",
   "language": "python",
   "name": "tf20a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
